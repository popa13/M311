\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{enumitem}
\usepackage[margin=2cm]{geometry}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{multicol}

\usepackage{comment}
\usepackage{url}
\usepackage{calc}
\usepackage{subcaption}
\usepackage{circledsteps}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{systeme}
\sysdelim..

\setlength\parindent{0pt}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{2pt}
\renewcommand{\footrulewidth}{0pt}
\rfoot{\thepage}
\lhead{\textsc{Math} 311}
\chead{\textsc{Homework 11}}
\rhead{Spring 2024}

\pgfplotsset{compat=1.16}

% MATH commands
\newcommand{\ga}{\left\langle}
\newcommand{\da}{\right\rangle}
\newcommand{\oa}{\left\lbrace}
\newcommand{\fa}{\right\rbrace}
\newcommand{\oc}{\left[}
\newcommand{\fc}{\right]}
\newcommand{\op}{\left(}
\newcommand{\fp}{\right)}

\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bF}{\mathbf{F}}

\newcommand{\ra}{\rightarrow}
\newcommand{\Ra}{\Rightarrow}

\newcommand{\sech}{\mathrm{sech}\,}
\newcommand{\csch}{\mathrm{csch}\,}
\newcommand{\curl}{\mathrm{curl}\,}
\newcommand{\dive}{\mathrm{div}\,}

\newcommand{\ve}{\varepsilon}
\newcommand{\spc}{\vspace*{0.5cm}}

\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Spn}{span}

\newcommand{\exo}[3]{\noindent\textcolor{red}{\fbox{\textbf{Section {#1} | Problem {#2}}}\hrulefill   \textbf{({#3} Pts})}\vspace*{10pt}}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\begin{document}
\thispagestyle{empty}
	\noindent \hrulefill \newline
	MATH-311 \hfill Pierre-Olivier Paris{\'e}\newline
	Homework 11 solutions \hfill Spring 2024\newline \vspace*{-0.7cm}

\noindent\hrulefill
	
	\spc

\exo{2.6}{1a}{10}

First we write
	\[
		\begin{bmatrix} 8 \\ 3 \\ 7 \end{bmatrix} = 2 \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix} + 3 \begin{bmatrix} 2 \\ 1 \\ 3 \end{bmatrix} .
	\]
Therefore, using the fact that $T$ is a linear transformation, we get
	\begin{align*}
		T \begin{bmatrix} 8 \\ 3 \\ 7 \end{bmatrix} = T \left( 2 \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix} + 3 \begin{bmatrix} 2 \\ 1 \\ 3 \end{bmatrix} \right) &= 2 \left( T \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix} \right) + 3 \left( T \begin{bmatrix} 2 \\ 1 \\ 3 \end{bmatrix} \right) \\ 
		&= 2 \begin{bmatrix} 2 \\ 3 \end{bmatrix} + 3 \begin{bmatrix} -1 \\ 0 \end{bmatrix} \\ 
		&= \begin{bmatrix} 1 \\ 6 \end{bmatrix} .
	\end{align*}

\spc 

\exo{2.6}{7a}{5}

Notice that
	\[
		T \left( \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} u \\ v \end{bmatrix} \right) = T \left( \begin{bmatrix} x + u \\ y + v \end{bmatrix} \right) = \begin{bmatrix} (x + u) (y + v) \\ 0 \end{bmatrix}
	\]
But,
	\[
		T \begin{bmatrix} x \\ y \end{bmatrix} + T \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} xy \\ 0 \end{bmatrix} + \begin{bmatrix} uv \\ 0 \end{bmatrix} = \begin{bmatrix} xy + uv \\ 0 \end{bmatrix} .
	\]
We have $(x + u) (y + v) = xy + xv + yu + yv \neq xy + uv$ in general. Therefore,
	\[
		T \left( \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} u \\ v \end{bmatrix} \right) \neq T \begin{bmatrix} x \\ y \end{bmatrix} + T \begin{bmatrix} u \\ v \end{bmatrix} 
	\]
The transformation $T$ does not satisfy Axiom T1, and is therefore not linear.

\spc 

\exo{2.6}{14a}{5}

There are many ways of doing that. Here are two ways:
	\begin{enumerate}
		\item Since $\mathbf{0} = \mathbf{0} + \mathbf{0}$ from the property of the zero vector, applying $T$ on both sides and use the linearity of $T$ gives
			\[
				T (\mathbf{0}) = T (\mathbf{0}) + T (\mathbf{0}) = 2 T (\mathbf{0}) \quad \Rightarrow \quad T(\mathbf{0}) - T (\mathbf{0}) = 2 T (\mathbf{0}) - T (\mathbf{0}) \quad \Rightarrow \quad \mathbf{0} = T (\mathbf{0}) .
			\]
		\item Recall the property $0 \mathbf{v} = \mathbf{0}$ for any vector $\mathbf{v}$. We therefore have $0 \mathbf{0} = \mathbf{0}$ and applying $T$ and using linearity of $T$:
			\[
				0 T (\mathbf{0}) = T (\mathbf{0}) .
			\]
		Again, using the property $0 \mathbf{v} = \mathbf{0}$, we get that $0 T (\mathbf{0})= \mathbf{0}$. Hence, $\mathbf{0} = T (\mathbf{0})$. 
	\end{enumerate}

\newpage

\exo{7.1}{4c}{10}

First, notice that $\{ x^2 , x + 1, x - 1 \}$ is a basis for $\mathbf{P_2}$. Notice that any polynomial $a + bx + cx^2$ can be written as
	\[
		a + bx + cx^2 = cx^2 + \big( \frac{a + b}{2} \big) (x  +1) + \big( \frac{a + b}{1} \big) (x - 1)
	\]
Hence,
	\[
		T (a + bx + cx^2) = \big( \frac{a + b}{2} \big) T (x + 1) + \big( \frac{a - b}{2} \big) T (x - 1) + c T (x^2) = \big( \frac{a - b}{2} \big) x + c x^3
	\]

For $\mathbf{v} = x^2 + x + 1$, we get
	\[
		T (\mathbf{v}) = \big( \frac{1 - 1}{2} \big) x + (1) x^3 = x^3 .
	\]

\textbf{Additional notes:} Notice that $T$ is not injective! We can show that $\ker T = \Spn \{ x + 1 \}$.

\spc

\exo{7.2}{1a}{15}

\textbf{Finding the kernel.} By definition $\ker T_A = \{ \mathbf{x} \, : \, T_A \mathbf{x} = \mathbf{0} \}$. We have
	\[
		\mathbf{x} \in \ker T_A \iff T_A \mathbf{x} = \mathbf{0} \iff \begin{bmatrix} 1 & 2 & -1 & 1\\3 & 1 & 0 & 2\\1 & -3 & 2 & 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ w \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} .
	\]
We find the RREF of $A$:
	\[
		A \rightarrow \begin{bmatrix} 1 & 0 & \frac{1}{5} & \frac{3}{5}\\0 & 1 & - \frac{3}{5} & \frac{1}{5}\\0 & 0 & 0 & 0 \end{bmatrix} .
	\]
Therefore we have $x = -z/5 - 3w/5$ and $y = 3z/5 - w/5$, with $z, w \in \mathbb{R}$.
	\[
		\mathbf{x} = \begin{bmatrix} x \\ y \\ z \\ w \end{bmatrix} = z \begin{bmatrix} -1/5 \\ 3/5 \\ 1 \\ 0 \end{bmatrix} + w \begin{bmatrix} -3/5 \\ -1/5 \\ 0 \\ 1 \end{bmatrix} = z \mathbf{x_1} + w \mathbf{x_2}
	\]
where $z, w \in \mathbb{R}$. Hence
	\[
		\ker T_A = \Spn \{ \mathbf{x_1} , \mathbf{x_2} \} = \Spn \Big\{ \begin{bmatrix} -\frac{1}{5} & \frac{3}{5} & 1 & 0 \end{bmatrix}^\top , \begin{bmatrix} -\frac{3}{5} & - \frac{1}{5} & 0 & 1 \end{bmatrix}^\top \Big\} .
	\]
This is a basis for $\ker T_A$ and therefore $\mathrm{nullity}\, T_A = 2$. 
\vspace*{10pt}

\textbf{Finding the image.} By definition $\Img T_A = \{ T_A (\mathbf{x}) \, : \, \mathbf{x} \in \mathbb{R}^4 \} = \{ A \mathbf{x} \, : \, \mathbf{x} \in \mathbb{R}^4 \}$. We have
	\begin{align*}
		A \mathbf{x} = \begin{bmatrix} 1 & 2 & -1 & 1\\3 & 1 & 0 & 2\\1 & -3 & 2 & 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ w\end{bmatrix} &= x \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix} + y \begin{bmatrix} 2 \\ 1 \\ -3 \end{bmatrix} + z \begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix} + w \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix} .
	\end{align*}
Since $x, y, z , w$ are arbitrary scalars, we see that
	\[
		\Img T_A = \Spn \left\{ \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix} , \begin{bmatrix} 2 \\ 1 \\ -3 \end{bmatrix}, \begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix} \right\} .
	\]
From the RREF of $A$, there is a pivot in the first and second columns of the RREF. Therefore, the first and second columns of $A$ forms a basis for $\Img T_A$. Hence
	\[
		\Img T_A = \Spn \left\{ \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix} , \begin{bmatrix} 2 \\ 1 \\ -3 \end{bmatrix} \right\} .
	\]
These two vectors form a basis for $\Img T_A$ and therefore $\mathrm{rank} \, T_A = 2$. 
\vspace*{10pt}

\textbf{Additional Notes:} There is a quicker way to find the rank of $T_A$ by using the Dimension Theorem. Since we know that $\mathrm{nullity} T_A = 2$ and $V = \mathbb{R}^4$, the Dimension Theorem tells us that
	\[
		\mathrm{nullity} \, T_A + \mathrm{rank} \, T_A = \dim V \quad \Rightarrow \quad 2 + \mathrm{rank}\, T_A = 4 \quad \Rightarrow \quad \mathrm{rank}\, T_A = 2 .
	\]

\spc

\exo{7.2}{20}{5}

Let $T : \mathbf{M_{nn}} \ra \mathbf{M_{nn}}$ be the linear transformation
	\[
		T (A) = A - A^\top .
	\]
From the Example done in the lecture notes, we know that
	$$
		\ker T = \{ A \in \mathbf{M_{nn}} \, : \, A \text{ is symmetric} \} = U \quad \text{ and } \quad \Img T = \{ A \in \mathbf{M_{nn}} \, : \, A \text{ is skew-symmetric}\} = V.
	$$

Therefore, by the Dimension Theorem, with $V = \mathbf{M_{nn}}$ we get
	\[
		\mathrm{nullity} \, T + \mathrm{rank} \, T = \dim (\mathbf{M_{nn}} ) .
	\]
We have $\mathrm{nullity} \, T = \dim (\ker T) = \dim U$, $\mathbf{rank} \, T = \dim (\Img T)$, and $\dim (\mathbf{M_{nn}}) = n^2$. Therefore, replacing all the data in the formula from the Dimension Theorem, we get
	\[
		\dim U + \dim V = n^2 .
	\]


\end{document}
	